---
title: "Tidy Modeling with R Notes"
subtitle: "STAT 359"
author: "Arturo Fonseca"

format:
  html:
    toc: true
    embed-resources: true
    link-external-newwindow: true
    
execute:
  warning: false
  eval: false

from: markdown+emoji
---

## 1: Software for modeling

## 2: A Tidyverse Primer

## 3: A Review of R Modeling Fundamentals

## 4: The Ames Housing Data

-   Log transform your data using `log10()`

-   Always perform exploratory data analysis, period

## 5: Spending our Data

-   "...we can think of as an available data budget."

**Splitting data**

-   Into the training set and test set

-   `initial_split()` returning an `rsplit` object

    -   To access each partition, use `training(<rsplit>)`,...

-   Use stratified sampling to avoid having class imbalance

    -   Use the `strata` argument within `initial_split` to stratify data

-   Use `initial_time_split()` instead of `initial_split()` to split time series data, using the latest data as the test set

**Validation set**

-   Use `initial_validation_split()` to split data into training/validation/testing split

**Multilevel data**

-   Safe to assume that data from a property are independent of other properties

**In general**

-   Separate training set from test set

### Introduction to Modeling

**Types of models**

-   Descriptive

    -   Describe characteristics of some data

-   Inferential

    -   Produce a decision

-   Predictive

    -   Produce accurate prediction

**Parametric vs. Nonparametric**

-   Parametric:

    -   Assumes `y` follows some known form/equation

    -   Estimate a set of parameters

-   Nonparametric

    -   "Empirically-driven model"

    -   No assumptions about form of `f(x)`

**Supervised vs. Unsupervised learning**

**Interpretability vs. Flexibility trade off**

-   Bias decreases as flexibility increases

### Linear Regression Overview

$\hat{y}=\hat{\beta_0}+\hat{\beta}_1x_1+\dots+\hat{\beta}_px_p$

-   Regression, mechanistic, parametric, supervised

-   $p$ predictors, each with a coefficient $\beta_i$

    -   Least squares typically used to calculate residuals $J(\sum_i(y_i-\hat{y}_i)^2$

-   Performance metric use RMSE instead $\sqrt{\frac{\sum_i(y_i-\hat{y}_i)^2}{n}}$

-   Closed-form solution: $\beta=(X^TX)^{-1}X^Ty$ by setting gradient to 0

### Fitting Models with Tidymodels

-   First step: training and test split

    -   Okay to use 20% for testing with large datasets

`palmerpenguins` package

```{r}
library(tidyverse)

# Apply transformation to data to reduce variance and impact of outliers
set.seed(1)
penguins_split <- rsample::initial_split(palmerpenguins::penguins, 0.8, body_mass_g)
penguins_train <- rsample::training(penguins_split)
penguins_test <- rsample::testing(penguins_split)

lm_mod <- parsnip::linear_reg() %>%
  parsnip::set_engine("lm")

penguins_recipe <- recipes::recipe(body_mass_g ~ flipper_length_mm + species,
                          data = penguins_train)

lm_workflow <- workflows::workflow() |>
  workflows::add_recipe(penguins_recipe) |>
  workflows::add_model(lm_mod)

lm_fit <- parsnip::fit(lm_workflow, penguins_train)
lm_fit

penguin_pred <- penguins_test |>
  bind_cols(predict(lm_fit, penguins_test))

penguin_pred |>
  yardstick::rmse(body_mass_g, .pred)
```

### Logistic Regression Overview

-   Classification, mechanistic

-   Result is a number (probability) between 0 and 1

-   ROC AUC is the area under the curve (between 0 and 1)

-   Accuracy is a number between 0 and 1

-   Sensitivity: ability to designate an event as +

-   Specificity: ability to designate observations as -

### Logistic Regression Code

-   To handle severe class imbalance, you can downsample or upsample on the training dataset only

```{r}
logistic_mod <- parsnip::logistic_reg() |>
  parsnip::set_engine("glm") |>
  parsnip::set_mode("classification")

penguins_recipe <- recipes::recipe(sex ~ ., data = penguins_train)

logistic_wrkflw <- workflows::workflow() |>
  workflows::add_model(logistic_mod) |>
  workflows::add_recipe(penguins_recipe)

logistic_fit <- parsnip::fit(logistic_wrkflw, penguins_train)

logistic_fit
```

## 7: A Model Workflow

> However, the model may need to be more focused on reducing false positive results (i.e., where true nonevents are classified as events). One way to do this is to raise the cutoff from 50% to some greater value

```{r}
#| eval: false
#| exec: false
lm_model <- 
  linear_reg() %>% 
  set_engine("lm")
# A workflow always requires a parsnip model object
lm_wflow <- 
  workflow() %>% 
  add_model(lm_model)
# Simple preprocessor
lm_wflow <- 
  lm_wflow %>% 
  add_formula(Sale_Price ~ Longitude + Latitude)
lm_wflow
#> ══ Workflow ═════════════════════════════════════════════════════════════════════════
#> Preprocessor: Formula
#> Model: linear_reg()
#> 
#> ── Preprocessor ─────────────────────────────────────────────────────────────────────
#> Sale_Price ~ Longitude + Latitude
#> 
#> ── Model ────────────────────────────────────────────────────────────────────────────
#> Linear Regression Model Specification (regression)
#> 
#> Computational engine: lm
# Workflows have a fit() method that can be used to create the 
lm_fit <- fit(lm_wflow, ames_train)
# We can also predict() on the fitted workflow:
predict(lm_fit, ames_test %>% slice(1:3))
# Both the model and preprocessor can be removed or updated:
lm_fit %>% update_formula(Sale_Price ~ Longitude)
# (Will remove old model)
# Use the add_variables() function
lm_wflow <- 
  lm_wflow %>% 
  remove_formula() %>% 
  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))
# Could also do
predictors = c(ends_with("tude"))
predictors = everything() # will exclude outcome columns
```

> Since the preprocessing is model dependent, **workflows** attempts to emulate what the underlying model would do whenever possible

Can use `library(lme4) \ lmer(distance ~ Sex + (age | Subject), data = Orthodont)`

## 8: Feature Engineering with recipes

> Other examples of preprocessing to build better features for modeling include:
>
> -   Correlation between predictors can be reduced via feature extraction or the removal of some predictors.
>
> -   When some predictors have missing values, they can be imputed using a sub-model.
>
> -   Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation.

> A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps via `step_*()` functions without immediately executing them; it is only a specification of what should be done

```{r}
#| eval: false
#| exec: false
lm_wflow <- 
  lm_wflow %>% 
  remove_variables() %>% 
  add_recipe(simple_ames)
lm_wflow
#> ══ Workflow ═════════════════════════════════════════════════════════════════════════
#> Preprocessor: Recipe
#> Model: linear_reg()
#> 
#> ── Preprocessor ─────────────────────────────────────────────────────────────────────
#> 2 Recipe Steps
#> 
#> • step_log()
#> • step_dummy()
#> 
#> ── Model ────────────────────────────────────────────────────────────────────────────
#> Linear Regression Model Specification (regression)
#> 
#> Computational engine: lm

# To tidy the model fit: 
lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy() %>% 
  slice(1:5)
#> # A tibble: 5 × 5
#>   term                       estimate std.error statistic   p.value
#>   <chr>                         <dbl>     <dbl>     <dbl>     <dbl>
#> 1 (Intercept)                -0.669    0.231        -2.90 3.80e-  3
#> 2 Gr_Liv_Area                 0.620    0.0143       43.2  2.63e-299
#> 3 Year_Built                  0.00200  0.000117     17.1  6.16e- 62
#> 4 Neighborhood_College_Creek  0.0178   0.00819       2.17 3.02e-  2
#> 5 Neighborhood_Old_Town      -0.0330   0.00838      -3.93 8.66e-  5
```

> First, when calling `recipe(..., data)`, the data set is used to determine the data types of each column so that selectors such as `all_numeric()` or `all_numeric_predictors()` can be used.
>
> Second, when preparing the data using `fit(workflow, data)`, the training data are used for all estimation operations including a recipe that may be part of the `workflow`, from determining factor levels to computing PCA components and everything in between.
>
> Finally, when using `predict(workflow, new_data)`, no model or preprocessor parameters like those from recipes are re-estimated using the values in `new_data`. Take centering and scaling using `step_normalize()` as an example. Using this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples at prediction time are standardized using these values from training when `predict()` is invoked.

How are interactions specified in a recipe? A base R formula would take an interaction using a `:`, so we would use:

```         
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + 
  log10(Gr_Liv_Area):Bldg_Type
# or
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type
```

The additional step would look like `step_interact(~ interaction terms)` where the terms on the right-hand side of the tilde are the interactions. These can include selectors, so it would be appropriate to use:

```         
simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  # Gr_Liv_Area is on the log scale from a previous step
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") )
```

In **recipes**, multiple steps can create these types of terms. To add a natural spline representation for this predictor:

```         
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, deg_free = 20)
```

...a recipe step for PCA might look like:

```         
  # Use a regular expression to capture house size predictors: 
  step_pca(matches("(SF$)|(Gr_Liv)"))
```

Note that all of these columns are measured in square feet. PCA assumes that all of the predictors are on the same scale. That’s true in this case, but often this step can be preceded by `step_normalize()`, which will center and scale each column.

Other step functions are row-based as well: `step_filter()`, `step_sample()`, `step_slice()`, and `step_arrange()`. In almost all uses of these steps, the `skip` argument should be set to `TRUE` (where `skip` refers to only applying these transformations to the training set).

------------------------------------------------------------------------

To solve this, the `add_role()`, `remove_role()`, and `update_role()` functions can be helpful. For example, for the house price data, the role of the street address column could be modified using:

```         
ames_rec %>% update_role(address, new_role = "street address")
```

After this change, the `address` column in the dataframe will no longer be a predictor but instead will be a `"street address"` according to the recipe.

### Random Forest Overview

-   Non-parametric, classification or regression, versatile and simple

-   Tuning parameters

    -   mtry: num of predictors to consider ($\sqrt{|\text{predictors}|}$)
    -   num of trees
    -   $\min n$

-   Decision tree method:

    -   Stratifying/segmenting the predictor space into simple regions
    -   Predictors use mean/mode response

### Workflows and Recipes

-   Explore your outcome variable

-   Split the data

-   Pre-process and explore a subset of the training data

-   Make the recipe - define your formula and add feature engineering steps

-   Define your model speculations - set your engine and mode

-   Set the workflow

-   Fit your model

-   Evaluate your model

Feature Engineering:

-   Manipulation of your data set to improve model training

Recipe:

-   Prepares your data for modeling

-   Pipeable sequences of feature engineering steps

-   Missingness:

    -   `step_impute_*()` (`knn`, `linear`, `mean`, `median`, `mode`, ...)
    -   If \<20% of a variable is missing, go ahead and impute it

-   Transformations:

    -   `step_*()` (`normalize`, ...)

-   Dummy

    -   Set character/factor variables as dummy variables

-   Remove unimportant variables

    -   `step_rm()` (MAKE SURE TO REMOVE BEFORE MAKING NEW DUMMY COLUMNS)

-   Other

    -   `set_nzv()`, `step_date()`

Model and workflow

-   Define model

-   Workflow objects can bind a model and a preprocessor

Predict and evaluate

-   Using a `metric_set()`

```{r}
#| eval: false
#| exec: false
penguins_train <- recipe(body_mass_g ~ ., data = penguin_train)
```

## 10: Resampling for Evaluating Performance

**10.1: The Resubstitution Approach**

Both RMSE and R2R2 are computed. The resubstitution statistics are:

```{r}
#| eval: false

estimate_perf(rf_fit, ames_train)
#> # A tibble: 2 × 4
#>   .metric .estimate object data 
#>   <chr>       <dbl> <chr>  <chr>
#> 1 rmse       0.0365 rf_fit train
#> 2 rsq        0.960  rf_fit train
estimate_perf(lm_fit, ames_train)
#> # A tibble: 2 × 4
#>   .metric .estimate object data 
#>   <chr>       <dbl> <chr>  <chr>
#> 1 rmse       0.0754 lm_fit train
#> 2 rsq        0.816  lm_fit train
```

::: callout-note
In this context, *bias* is the difference between the true pattern or relationships in data and the types of patterns that the model can emulate. Many black-box machine learning models have low bias, meaning they can reproduce complex relationships. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and are considered *high bias* models
:::

**10.2 Resampling Methods**

> A bootstrap sample of the training set is a sample that is the same size as the training set but is drawn *with replacement*.

::: callout-note
For some resampling methods, such as the bootstrap or repeated cross-validation, there will be multiple predictions per row of the original training set. To obtain summarized values (averages of the replicate predictions) use collect_predictions(object, summarize = TRUE).
:::

```{r}
#| eval: false

library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```

## 11: Comparing Models with Resampling

```{r}
#| eval: false

lm_models <- 
  lm_models %>% 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1101, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = ames_folds, control = keep_pred)
```

### Resampling and KNN Overview

**KNN**

-   Regression or classification
-   Data-driven
-   Tuning parameter: $k$ neighbors
-   "Nearest" dependent on distance metric: $\text{distance}=\sqrt{\sum_{j=1}^m (y_j-x_j)^2}$

Bias-variance trade-off

![](images/Screenshot%202024-08-05%20at%202.23.19%20PM.png)

**Resampling**

-   The selection of randomized cases from the original data sample with replacement in order to estimate the population parameter multiple times

**Types of resampling**

-   **V-fold cross-validation**

    -   Partition data into V sets of roughly equal size

    -   V sets of performance metrics

-   **Leave-one-out cross validation**

-   **Monte Carlo cross-validation**

    -   Data sets are not mutually exclusive

**Where does this fit?**

1.  Split data
2.  EDA and feature engineering
3.  Fold the training data
4.  Define the recipes
5.  Set your engines and create workflows
6.  Fit model(s) to the folded data
